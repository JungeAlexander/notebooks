{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b941b5-baf1-4201-ba36-eeca1b8f1a03",
   "metadata": {},
   "source": [
    "# Chapter 3. Transformer Anatomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a169426-d387-44a7-bdbf-822fb6259815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "setup_chapter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6ad6f-067b-43be-b079-b2402af51255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a433a2-c877-489e-8a39-b5141c6ebae8",
   "metadata": {},
   "source": [
    "## Visualization using BERTViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c540d300-d646-42e5-af09-bcabecdd7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz.neuron_view import show\n",
    "from bertviz.transformers_neuron_view import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfe94b-47ba-43a9-8d88-7ae92cba4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = BertModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a2491-ab81-488b-b013-06eabaac44b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"time flies like an arrow\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037413dd-51e7-4a60-a792-714efe41652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dee9e5-6b33-4654-ae4d-ec067e688296",
   "metadata": {},
   "source": [
    "Supermarket analogy when shopping for a dinner recipe to understand query, key, value:\n",
    "\n",
    "- ingredient - query\n",
    "- labels of grocereries in shelves - keys\n",
    "- items on shelves - value\n",
    "- BUT: smooth ingredients might lead to buy 1L milk, joghurt and cheese when looking for 2L of milk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3773ccd-c742-4a9a-8dbb-62957595ef09",
   "metadata": {},
   "source": [
    "## Implementing scaled dot-product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833eaa20-cabc-47d8-962e-c4c77610fe57",
   "metadata": {},
   "source": [
    "![](https://github.com/JungeAlexander/notebooks/raw/3832388ce6b0ff7868f2b4dc15c6105a29c21fc3//images/chapter03_attention-ops.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acb392-711f-42d8-9776-e85806f38a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"bert-base-uncased\"\n",
    "text = \"time flies like an arrow\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8932bbd-0f6b-4e21-91e2-4c66ea96a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(text, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541797a8-7253-433c-8bfc-997918c9d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aabbb4-7fb3-416e-bfa4-f7e0dd59211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7a2891-0dbe-4366-98e5-ffb315b1e505",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_emb = nn.Embedding(config.vocab_size, config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9f76f-991a-4c4f-8d42-8dfc451a45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbeb623-ba93-495b-a265-634ebca9f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds = token_emb(inputs.input_ids)\n",
    "inputs_embeds.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57690e-22d3-4fa1-b3d0-e81f8c9c1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds[0, :, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8aa597-d4f7-4571-8ce2-cd07ecb8ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_embeds.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2d12f-ab43-4519-8212-1b8b2cbdf420",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs_embeds\n",
    "key = inputs_embeds\n",
    "value = inputs_embeds\n",
    "\n",
    "dim_k = key.size(-1)\n",
    "\n",
    "scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "scores.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebec8be-0ce1-40c1-bb31-a2ef85e9bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key.transpose(1, 2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c079279c-d3c4-42e1-9b04-58f38f35040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = F.softmax(scores, dim=-1)\n",
    "weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b21d89-ba18-4fcb-83ea-ed2747a9a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e97e45-13a0-4004-804c-b90adf3118ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value):\n",
    "    dim_k = key.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e80952-6de5-4fda-95c8-74d583cef9a4",
   "metadata": {},
   "source": [
    "## Implementing multi-headed attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd160d7e-1316-4b8c-a47b-417a174292a4",
   "metadata": {},
   "source": [
    "![](https://github.com/JungeAlexander/notebooks/raw/3832388ce6b0ff7868f2b4dc15c6105a29c21fc3//images/chapter03_multihead-attention.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466a3c9c-cb96-45ea-9da7-5df64bf1615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        return scaled_dot_product_attention(\n",
    "            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031974f-d70d-445d-8b55-2f6c59ee3f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        num_heads = config.num_attention_heads\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        x = self.output_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bad06-667f-4474-8572-987f5f53a13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = MultiHeadAttention(config)\n",
    "attn_output = multihead_attn(inputs_embeds)\n",
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d86af1-51d4-4dc7-be50-ffbc661d59ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view\n",
    "\n",
    "model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n",
    "\n",
    "sentence_a = \"time flies like an arrow\"\n",
    "\n",
    "sentence_b = \"fruit flies like a banana\"\n",
    "\n",
    "viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors=\"pt\")\n",
    "attention = model(**viz_inputs).attentions\n",
    "sentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\n",
    "tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n",
    "\n",
    "head_view(attention, tokens, sentence_b_start, heads=[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b679acd-c38e-44b4-af12-6d9fa0a82fe9",
   "metadata": {},
   "source": [
    "## The Feed-Forward Layer (position-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f0c48e-6fab-4d92-a95f-276bdcaa71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0d7ac-3052-4d6f-9169-237207ca1aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward = FeedForward(config)\n",
    "ff_outputs = feed_forward(attn_output)\n",
    "ff_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3499214b-80db-40d3-b599-dd8e051dc960",
   "metadata": {},
   "source": [
    "## Adding Layer Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18ccc7c-bbf4-4703-a963-45fba197b50c",
   "metadata": {},
   "source": [
    "![](https://github.com/JungeAlexander/notebooks/raw/cf30f13ae7bf88b0b241bd89b29e385707c6e38e//images/chapter03_layer-norm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488b6bb-1009-49b5-85a8-8811ec8f4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.feed_forward = FeedForward(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.layer_norm_1(x))\n",
    "        x = x + self.feed_forward(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff977bac-8ba7-4ef0-ac73-23ffb042bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = TransformerEncoderLayer(config)\n",
    "encoder_layer(inputs_embeds).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d853df9-4acd-41db-a7b2-e556da3c7ec1",
   "metadata": {},
   "source": [
    "## Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46cbdf-2310-4d93-beda-6ea7f5e1b2c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-with-transformers-notebooks",
   "language": "python",
   "name": "nlp-with-transformers-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
